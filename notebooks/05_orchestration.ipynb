{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 05 - Orchestration LLM\n",
    "\n",
    "**Objectif** : Combiner tous les composants pour r√©pondre en langage naturel\n",
    "\n",
    "**Flow** :\n",
    "1. Question utilisateur\n",
    "2. Recherche vectorielle Mediatech ‚Üí datasets pertinents\n",
    "3. MCP datagouv ‚Üí donn√©es fra√Æches\n",
    "4. LLM Albert ‚Üí r√©ponse avec sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Configuration et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import duckdb\n",
    "import httpx\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "ALBERT_API_KEY = os.getenv(\"ALBERT_API_KEY\")\n",
    "ALBERT_API_URL = os.getenv(\"ALBERT_API_URL\", \"https://albert.api.etalab.gouv.fr/v1\")\n",
    "MCP_URL = os.getenv(\"MCP_DATAGOUV_URL\", \"https://mcp.data.gouv.fr/mcp\")\n",
    "\n",
    "print(f\"‚úÖ Albert API : {ALBERT_API_URL}\")\n",
    "print(f\"‚úÖ MCP URL : {MCP_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Charger Mediatech (recherche vectorielle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Charger les donn√©es Mediatech\n",
    "PARQUET_GLOB = \"../huggingface/data_gouv_datasets_catalog_part_*.parquet\"\n",
    "con = duckdb.connect()\n",
    "\n",
    "df = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        doc_id,\n",
    "        title,\n",
    "        organization,\n",
    "        description,\n",
    "        url,\n",
    "        quality_score,\n",
    "        \"embeddings_bge-m3\" as embedding_json\n",
    "    FROM read_parquet('{PARQUET_GLOB}')\n",
    "    WHERE \"embeddings_bge-m3\" IS NOT NULL\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"‚úÖ {len(df):,} datasets charg√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Parser et normaliser les embeddings\n",
    "embeddings_list = [json.loads(e) for e in df[\"embedding_json\"]]\n",
    "embeddings_matrix = np.array(embeddings_list, dtype=np.float32)\n",
    "\n",
    "# Pr√©-normaliser pour la similarit√© cosinus\n",
    "norms = np.linalg.norm(embeddings_matrix, axis=1, keepdims=True)\n",
    "embeddings_normalized = embeddings_matrix / norms\n",
    "\n",
    "print(f\"‚úÖ Embeddings : {embeddings_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Fonctions Albert API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Obtenir l'embedding BGE-M3 d'un texte via Albert API.\n",
    "    \"\"\"\n",
    "    url = f\"{ALBERT_API_URL}/embeddings\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {ALBERT_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": \"BAAI/bge-m3\",\n",
    "        \"input\": text\n",
    "    }\n",
    "    \n",
    "    with httpx.Client(timeout=30) as client:\n",
    "        response = client.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "    data = response.json()\n",
    "    embedding = data[\"data\"][0][\"embedding\"]\n",
    "    return np.array(embedding, dtype=np.float32)\n",
    "\n",
    "\n",
    "# Mod√®le LLM disponible sur Albert API\n",
    "LLM_MODEL = \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\"  # alias: albert-large\n",
    "\n",
    "\n",
    "def chat_completion(messages: list, model: str = LLM_MODEL) -> str:\n",
    "    \"\"\"\n",
    "    Appeler le LLM Albert pour g√©n√©rer une r√©ponse.\n",
    "    \"\"\"\n",
    "    url = f\"{ALBERT_API_URL}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {ALBERT_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.3,\n",
    "        \"max_tokens\": 1024\n",
    "    }\n",
    "    \n",
    "    with httpx.Client(timeout=60) as client:\n",
    "        response = client.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "    data = response.json()\n",
    "    return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "# Test\n",
    "print(f\"ü§ñ Mod√®le LLM : {LLM_MODEL}\")\n",
    "test_response = chat_completion([{\"role\": \"user\", \"content\": \"Dis bonjour en une phrase.\"}])\n",
    "print(f\"‚úÖ LLM test : {test_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Recherche vectorielle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_datasets(query: str, top_k: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Recherche s√©mantique dans Mediatech.\n",
    "    \"\"\"\n",
    "    # Embedding de la requ√™te\n",
    "    query_embedding = get_embedding(query)\n",
    "    query_norm = query_embedding / np.linalg.norm(query_embedding)\n",
    "    \n",
    "    # Similarit√© cosinus\n",
    "    similarities = embeddings_normalized @ query_norm\n",
    "    \n",
    "    # Top-k\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        row = df.iloc[idx]\n",
    "        results.append({\n",
    "            \"doc_id\": row[\"doc_id\"],\n",
    "            \"title\": row[\"title\"],\n",
    "            \"organization\": row[\"organization\"],\n",
    "            \"description\": row[\"description\"][:500] if row[\"description\"] else \"\",\n",
    "            \"url\": row[\"url\"],\n",
    "            \"similarity\": float(similarities[idx])\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test\n",
    "results = search_datasets(\"qualit√© de l'air\", top_k=3)\n",
    "for r in results:\n",
    "    print(f\"[{r['similarity']:.3f}] {r['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Client MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCPClient:\n",
    "    \"\"\"Client MCP simplifi√©.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str):\n",
    "        self.base_url = base_url\n",
    "        self.session_id = None\n",
    "        self._request_id = 0\n",
    "    \n",
    "    def _call(self, method: str, params: dict = None) -> dict:\n",
    "        self._request_id += 1\n",
    "        payload = {\n",
    "            \"jsonrpc\": \"2.0\",\n",
    "            \"id\": self._request_id,\n",
    "            \"method\": method,\n",
    "            \"params\": params or {}\n",
    "        }\n",
    "        \n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Accept\": \"application/json, text/event-stream\"\n",
    "        }\n",
    "        if self.session_id:\n",
    "            headers[\"Mcp-Session-Id\"] = self.session_id\n",
    "        \n",
    "        with httpx.Client(timeout=60, http2=False) as client:\n",
    "            response = client.post(self.base_url, json=payload, headers=headers)\n",
    "            if \"mcp-session-id\" in response.headers:\n",
    "                self.session_id = response.headers[\"mcp-session-id\"]\n",
    "            response.raise_for_status()\n",
    "        \n",
    "        # Parser SSE si n√©cessaire\n",
    "        if \"text/event-stream\" in response.headers.get(\"content-type\", \"\"):\n",
    "            for line in response.text.split(\"\\n\"):\n",
    "                if line.startswith(\"data:\"):\n",
    "                    data = line[5:].strip()\n",
    "                    if data:\n",
    "                        return json.loads(data).get(\"result\", {})\n",
    "            return {}\n",
    "        \n",
    "        return response.json().get(\"result\", {})\n",
    "    \n",
    "    def initialize(self):\n",
    "        return self._call(\"initialize\", {\n",
    "            \"protocolVersion\": \"2024-11-05\",\n",
    "            \"capabilities\": {},\n",
    "            \"clientInfo\": {\"name\": \"poc-datagouv\", \"version\": \"0.1.0\"}\n",
    "        })\n",
    "    \n",
    "    def call_tool(self, name: str, arguments: dict = None) -> str:\n",
    "        \"\"\"Appelle un tool et retourne le texte de r√©ponse.\"\"\"\n",
    "        result = self._call(\"tools/call\", {\"name\": name, \"arguments\": arguments or {}})\n",
    "        if result.get(\"content\"):\n",
    "            content = result[\"content\"][0]\n",
    "            if content.get(\"type\") == \"text\":\n",
    "                return content[\"text\"]\n",
    "        return \"\"\n",
    "\n",
    "# Initialiser\n",
    "mcp = MCPClient(MCP_URL)\n",
    "mcp.initialize()\n",
    "print(f\"‚úÖ MCP connect√© (session: {mcp.session_id[:8]}...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. Assistant complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"Tu es un assistant sp√©cialis√© dans les donn√©es publiques fran√ßaises (data.gouv.fr).\n",
    "\n",
    "Tu r√©ponds aux questions en te basant sur les datasets fournis dans le contexte.\n",
    "- Sois pr√©cis et factuel\n",
    "- Cite tes sources (titre du dataset, organisation)\n",
    "- Si tu ne trouves pas l'information, dis-le clairement\n",
    "- R√©ponds en fran√ßais\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def ask(question: str, top_k: int = 3, fetch_mcp: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Pipeline complet : question ‚Üí recherche ‚Üí enrichissement ‚Üí r√©ponse.\n",
    "    \n",
    "    Args:\n",
    "        question: Question en langage naturel\n",
    "        top_k: Nombre de datasets √† r√©cup√©rer\n",
    "        fetch_mcp: Si True, enrichit avec les donn√©es MCP\n",
    "    \"\"\"\n",
    "    print(f\"üîç Recherche de datasets pertinents...\")\n",
    "    \n",
    "    # 1. Recherche vectorielle\n",
    "    datasets = search_datasets(question, top_k=top_k)\n",
    "    \n",
    "    # 2. Construire le contexte\n",
    "    context_parts = []\n",
    "    \n",
    "    for i, ds in enumerate(datasets, 1):\n",
    "        part = f\"\"\"### Dataset {i}: {ds['title']}\n",
    "- Organisation: {ds['organization']}\n",
    "- URL: {ds['url']}\n",
    "- Score de pertinence: {ds['similarity']:.2f}\n",
    "- Description: {ds['description'][:300]}...\"\"\"\n",
    "        \n",
    "        # Enrichir avec MCP si demand√©\n",
    "        if fetch_mcp:\n",
    "            try:\n",
    "                # Essayer de r√©cup√©rer plus d'infos via MCP\n",
    "                mcp_info = mcp.call_tool(\"get_dataset_info\", {\"dataset_id\": ds[\"doc_id\"]})\n",
    "                if mcp_info:\n",
    "                    part += f\"\\n- Infos MCP:\\n{mcp_info[:500]}\"\n",
    "            except:\n",
    "                pass  # Ignorer les erreurs MCP\n",
    "        \n",
    "        context_parts.append(part)\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # 3. Appeler le LLM\n",
    "    print(f\"ü§ñ G√©n√©ration de la r√©ponse...\")\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"Contexte (datasets trouv√©s sur data.gouv.fr) :\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Question : {question}\n",
    "\n",
    "R√©ponds de mani√®re concise en citant les sources.\"\"\"}\n",
    "    ]\n",
    "    \n",
    "    response = chat_completion(messages)\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "print(\"‚úÖ Fonction ask() pr√™te\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 7. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1\n",
    "question = \"O√π trouver les donn√©es sur les bornes de recharge √©lectrique ?\"\n",
    "print(f\"‚ùì {question}\\n\")\n",
    "\n",
    "response = ask(question, top_k=3, fetch_mcp=False)\n",
    "print(f\"\\nüí¨ R√©ponse :\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2 - avec enrichissement MCP\n",
    "question = \"O√π trouver les donn√©es sur les bornes de recharge √©lectrique ?\"\n",
    "print(f\"‚ùì {question}\\n\")\n",
    "\n",
    "response = ask(question, top_k=3, fetch_mcp=True)\n",
    "print(f\"\\nüí¨ R√©ponse :\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3\n",
    "question = \"Quels sont les jeux de donn√©es les plus populaires sur les transports en commun ?\"\n",
    "print(f\"‚ùì {question}\\n\")\n",
    "\n",
    "response = ask(question, top_k=5, fetch_mcp=False)\n",
    "print(f\"\\nüí¨ R√©ponse :\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 8. R√©sum√©\n",
    "\n",
    "**Pipeline impl√©ment√©** :\n",
    "```\n",
    "Question ‚Üí Embedding (Albert) ‚Üí Recherche vectorielle (Mediatech)\n",
    "    ‚Üí Enrichissement (MCP) ‚Üí G√©n√©ration r√©ponse (Albert LLM)\n",
    "```\n",
    "\n",
    "**Fonction principale** : `ask(question, top_k, fetch_mcp)`\n",
    "\n",
    "---\n",
    "\n",
    "## Prochaine √©tape\n",
    "\n",
    "**Notebook 06** : Interface utilisateur finale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
